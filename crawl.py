import requests
import json
import time
import os
from datetime import datetime

# ================= é…ç½®åŒºåŸŸ =================
# æŠ“å–é—´éš” (2å°æ—¶)
INTERVAL = 7200 

# å®šä¹‰å››ä¸ªåˆ†ç±»çš„æ•°æ®æ–‡ä»¶å
FILES = {
    "finance": "data_finance.json",  # è´¢ç»
    "tech": "data_tech.json",       # ç§‘æŠ€
    "global": "data_global.json",   # å›½é™…
    "general": "data_general.json"  # ç»¼åˆ/å¨±ä¹
}

# ================= æ ¸å¿ƒåˆ†ç±»å­—å…¸ =================
# åœ¨è¿™é‡Œå®šä¹‰å“ªäº› ID å±äºå“ªä¸ªåˆ†ç±»
# ä½ å¯ä»¥æ ¹æ®éœ€è¦æŠŠ id ä»ä¸€ä¸ªåˆ—è¡¨ç§»åŠ¨åˆ°å¦ä¸€ä¸ªåˆ—è¡¨
CATEGORY_MAP = {
    "finance": [
        "wallstreetcn-hot", "wallstreetcn-news", "wallstreetcn-quick",
        "cls-hot", "cls-depth", "cls-telegraph",
        "xueqiu-hotstock", "gelonghui", "jin10", 
        "mktnews-flash", "fastbull-express", "fastbull-news"
    ],
    "tech": [
        "36kr-quick", "36kr-renqi", 
        "sspai", "coolapk", "ithome", "huxiu", 
        "geekpark", "qbitai", "producthunt", 
        "github-trending-today", "hackernews", "v2ex-share", 
        "freebuf", "solidot"
    ],
    "global": [
        "zaobao", "sputniknewscn", "cankaoxiaoxi", "kaopu"
    ],
    "general": [
        "zhihu", "weibo", "douyin", "baidu", 
        "bilibili-hot-search", "tieba", "toutiao", 
        "thepaper", "douban", "hupu", 
        "chongbuluo-hot", "chongbuluo-latest", "nowcoder"
    ]
}

# æŠŠæ‰€æœ‰ ID åˆå¹¶æˆä¸€ä¸ªå¤§åˆ—è¡¨ï¼Œç”¨æ¥å‘è¯·æ±‚
ALL_SOURCES = []
for ids in CATEGORY_MAP.values():
    ALL_SOURCES.extend(ids)

# ===========================================

def get_current_time():
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def run_spider():
    print(f"[{get_current_time()}] ğŸš€ å¼€å§‹æ–°ä¸€è½®æŠ“å–...")
    
    url = "https://newsnow.busiyi.world/api/s/entire"
    
    headers = {
        "content-type": "application/json",
        "origin": "https://newsnow.busiyi.world",
        "referer": "https://newsnow.busiyi.world/c/hottest",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36 Edg/143.0.0.0"
    }

    payload = { "sources": ALL_SOURCES }

    try:
        print("â³ æ­£åœ¨è¯·æ±‚æ•°æ®...")
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        
        if response.status_code == 200:
            raw_data = response.json()
            print(f"ğŸ“¦ æˆåŠŸæ”¶åˆ°æ•°æ®ï¼Œå¼€å§‹åˆ†ç±»å¤„ç†...")

            # åˆå§‹åŒ– 4 ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨æ¥è£…ä¸åŒåˆ†ç±»çš„æ•°æ®
            categorized_data = {
                "finance": [],
                "tech": [],
                "global": [],
                "general": []
            }

            # éå†åŸå§‹æ•°æ®ï¼Œè¿›è¡Œåˆ†æ‹£
            for platform in raw_data:
                site_id = platform.get('id')
                items = platform.get('items', [])
                
                if not items: continue

                # ã€æç®€å¤„ç†ã€‘åªä¿ç•™ title å’Œ url
                clean_items = []
                for item in items:
                    clean_items.append({
                        "title": item.get('title', '').strip(),
                        "url": item.get('url', '')
                    })

                # æ„å»ºç²¾ç®€åçš„å¹³å°å¯¹è±¡
                clean_platform = {
                    "id": site_id,
                    "items": clean_items
                }

                # åˆ¤æ–­è¿™ä¸ªå¹³å°å±äºå“ªä¸ªåˆ†ç±»ï¼Œæ‰”è¿›å¯¹åº”çš„åˆ—è¡¨
                found_category = False
                for cat_name, ids_list in CATEGORY_MAP.items():
                    if site_id in ids_list:
                        categorized_data[cat_name].append(clean_platform)
                        found_category = True
                        break
                
                # å¦‚æœæ²¡åœ¨å­—å…¸é‡Œå®šä¹‰çš„ï¼Œé»˜è®¤æ‰”è¿› general
                if not found_category:
                    categorized_data["general"].append(clean_platform)

            # å†™å…¥ 4 ä¸ªç‹¬ç«‹æ–‡ä»¶
            for cat_name, data_list in categorized_data.items():
                filename = FILES[cat_name]
                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(data_list, f, ensure_ascii=False, indent=2) # indent=2 ä¸ºäº†è®©ä½ æ‰“å¼€çœ‹æ—¶æ›´æ¸…æ™°
                print(f"âœ… å·²ç”Ÿæˆ: {filename} (åŒ…å« {len(data_list)} ä¸ªå¹³å°)")

        else:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")

    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

# ================= ä¸»ç¨‹åº =================
if __name__ == "__main__":
    print(f"ğŸ¤– åˆ†ç±»çˆ¬è™«å·²å¯åŠ¨ï¼")
    print(f"æ•°æ®å°†åˆ†åˆ«ä¿å­˜ä¸º: {', '.join(FILES.values())}")
    
    
    # Github Actions ç¯å¢ƒä¸‹åªè¿è¡Œä¸€æ¬¡
    if os.environ.get("GITHUB_ACTIONS"):
        run_spider()
        print("âš¡ GitHub Action ç¯å¢ƒï¼šå•æ¬¡è¿è¡Œç»“æŸã€‚")
    else:
        while True:
            run_spider()
            print(f"ğŸ˜´ ä¼‘æ¯ {INTERVAL} ç§’...")
            time.sleep(INTERVAL)
